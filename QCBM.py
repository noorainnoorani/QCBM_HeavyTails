import pennylane as qml
import numpy as np
import matplotlib.pyplot as plt

import jax
import jax.numpy as jnp

import optax

from functools import partial  # Importing 'partial' from functools to create partially evaluated functions

class QCBM:
    """
    A class representing a Quantum-Classical Boltzmann Machine (QCBM). 
    This class includes methods to compute the loss between a quantum circuit's output distribution 
    and a target probability distribution.
    """

    def __init__(self, circ, loss, py):
        """
        Initializes the QCBM model.

        Args:
        - circ (callable): A quantum circuit function that takes parameters and outputs a probability distribution.
        - loss (callable): A function to compute the loss between the output distribution of the quantum circuit 
                            and the target distribution.
        - py (callable): The target probability distribution π(x) that the quantum circuit is trying to approximate.
        """
        self.circ = circ  # Quantum circuit function that returns a probability distribution given certain parameters
        self.loss = loss  # Loss function (such as MMD, KL divergence, etc.) to compare the output of the quantum circuit with the target
        self.py = py      # The target probability distribution (π(x)) the quantum circuit aims to match

    @partial(jax.jit, static_argnums=0)  # Use JAX's JIT compilation for optimizing the 'loss_fn' method
    def loss_fn(self, params):
        """
        Computes the loss between the quantum circuit's output distribution and the target distribution.

        Args:
        - params (array-like): Parameters for the quantum circuit, which define the quantum state.

        Returns:
        - loss (float): The computed loss value between the quantum circuit's output distribution and the target distribution.
        - px (array): The output probability distribution generated by the quantum circuit.
        """
        px = self.circ(params)  # Compute the output distribution (px) from the quantum circuit using the current parameters
        return self.loss(px, self.py), px  # Return the loss between the output distribution px and the target distribution py, along with px itself

class MMD:
    """
    A class to compute the Maximum Mean Discrepancy (MMD) between two distributions
    using a kernel-based approach. The MMD measures the distance between two probability 
    distributions, commonly used in statistics and machine learning for comparing distributions.
    """

    def __init__(self, scales, space):
        """
        Initialize the MMD class with kernel scales and a sample space.

        Parameters:
        - scales (array-like): Bandwidths for the Gaussian kernels.
        - space (array-like): The space of points where the kernel matrix is computed.

        Attributes:
        - K (jnp.array): The combined kernel matrix computed over the sample space.
        - scales (array-like): The input scales for the kernels.
        """
        # Compute the gamma values (inverse variance) for Gaussian kernels
        gammas = 1 / (2 * (scales**2))
        
        # Compute pairwise squared distances in the sample space
        # This creates a distance matrix where sq_dists[i, j] = |space[i] - space[j]|^2
        sq_dists = jnp.abs(space[:, None] - space[None, :]) ** 2

        # Compute the kernel matrix as a sum of Gaussian kernels with different scales
        # Each kernel is weighted equally by dividing by the number of scales
        self.K = sum(jnp.exp(-gamma * sq_dists) for gamma in gammas) / len(scales)
        
        # Store the scales for reference
        self.scales = scales

    def k_expval(self, px, py):
        """
        Compute the kernel expectation value between two distributions.

        Parameters:
        - px (jnp.array): A distribution vector (e.g., probability values) over the sample space.
        - py (jnp.array): Another distribution vector over the sample space.

        Returns:
        - float: The kernel expectation value computed as px^T @ K @ py.
        """
        # Perform the matrix-vector multiplication to compute px^T @ K @ py
        return px @ self.K @ py

    def __call__(self, px, py):
        """
        Compute the MMD distance between two distributions px and py.

        Parameters:
        - px (jnp.array): A distribution vector over the sample space.
        - py (jnp.array): Another distribution vector over the sample space.

        Returns:
        - float: The MMD distance between px and py.
        """
        # Compute the difference between the distributions
        pxy = px - py
        
        # Use the kernel expectation value to compute the MMD distance
        # MMD is essentially the quadratic form of the difference vector with the kernel
        return self.k_expval(pxy, pxy)
    

class KLDivergence:
    def __init__(self):
        """
        Initializes the KLDivergence class. No specific parameters are needed since
        KL divergence is a function of two probability distributions.
        """
        pass

    def kl_term(self, p, q):
        """
        Computes the per-element KL divergence term: p * log(p / q).
        Assumes p and q are non-negative and sum to 1.
        
        Parameters:
        - p: Probability distribution (jnp.array)
        - q: Probability distribution (jnp.array)

        Returns:
        - KL divergence term: jnp.array
        """
        # Avoid division by zero and log of zero by using a small epsilon
        epsilon = 1e-10
        p_safe = jnp.clip(p, epsilon, 1.0)
        q_safe = jnp.clip(q, epsilon, 1.0)
        return p_safe * jnp.log(p_safe / q_safe)

    def __call__(self, p, q):
        """
        Computes the KL divergence between two distributions.

        Parameters:
        - p: Probability distribution (jnp.array)
        - q: Probability distribution (jnp.array)

        Returns:
        - KL divergence: float
        """
        return jnp.sum(self.kl_term(p, q))